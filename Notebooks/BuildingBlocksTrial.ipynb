{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "sys.path.append('..')\n",
    "from Framework.building_blocks import *\n",
    "from Framework import datasets\n",
    "from Framework.custom_sklearn_pipeline import *\n",
    "from Framework.utils import *\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import openml\n",
    "#from openml import datasets\n",
    "from openml import tasks\n",
    "from openml import runs\n",
    "from openml import flows\n",
    "from openml import extensions\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,name = datasets.returnDataset(8)\n",
    "X = preprocessing.StandardScaler().fit_transform(X)\n",
    "y = preprocessing.StandardScaler().fit_transform(y.reshape(-1,1))\n",
    "X = pd.DataFrame(X)\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.25,random_state = 42)\n",
    "X_train.reset_index(inplace = True)\n",
    "X_train.drop(['index'],axis = 1,inplace = True)\n",
    "X_test.reset_index(inplace = True)\n",
    "X_test.drop(['index'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_mode = 'Original'\n",
    "explainer_type = 'XGBoost'\n",
    "model_type = 'XGBoost'\n",
    "nClusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_blocks(elems = 4):\n",
    "    if elems == 5:\n",
    "        processing_block = ProcessingBlock()\n",
    "        explainer_block = ExplainerBlock(explainer_type)\n",
    "        reduce_block = ReduceBlock(PCA(1))\n",
    "        cluster_block = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "        ensemble_block = EnsembleBlock(model_type)\n",
    "        return processing_block,explainer_block,reduce_block,cluster_block,ensemble_block\n",
    "    else:\n",
    "        processing_block = ProcessingBlock()\n",
    "        explainer_block = ExplainerBlock(explainer_type)\n",
    "        cluster_block = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "        ensemble_block = EnsembleBlock(model_type)\n",
    "        return processing_block,explainer_block,cluster_block,ensemble_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_block = ProcessingBlock()\n",
    "explainer_block = ExplainerBlock('XGBoost')\n",
    "cluster_block = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "ensemble_block = EnsembleBlock(model_type)\n",
    "\n",
    "processing_block2 = ProcessingBlock()\n",
    "explainer_block2 = ExplainerBlock(explainer_type)\n",
    "cluster_block2 = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "ensemble_block2 = EnsembleBlock(model_type)\n",
    "\n",
    "processing_block3 = ProcessingBlock()\n",
    "explainer_block3 = ExplainerBlock(explainer_type)\n",
    "cluster_block3 = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "ensemble_block3 = EnsembleBlock(model_type)\n",
    "\n",
    "processing_block4 = ProcessingBlock()\n",
    "explainer_block4 = ExplainerBlock(explainer_type)\n",
    "reduce_block4 = ReduceBlock(PCA(nClusters))\n",
    "cluster_block4 = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "ensemble_block4 = EnsembleBlock(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X,X_test,y,y_test = train_test_split(X,y)\n",
    "#X_train,X_val,y_train,y_val = processing_block.split_data(X,y,test_split=0.25)\n",
    "#X.reset_index(inplace = True)\n",
    "#X.drop(['index'],axis = 1,inplace = True)\n",
    "#X_train.reset_index(inplace = True)\n",
    "#X_train.drop(['index'],axis = 1,inplace = True)\n",
    "#X_test.reset_index(inplace = True)\n",
    "#X_test.drop(['index'],axis = 1,inplace = True)\n",
    "processing_block4 = ProcessingBlock()\n",
    "explainer_block4 = ExplainerBlock(explainer_type)\n",
    "reduce_block4 = ReduceBlock(PCA(nClusters))\n",
    "cluster_block4 = ClusterBlock(nClusters,KMeans(n_clusters = nClusters,random_state = 0),KNeighborsClassifier(n_neighbors = nClusters))\n",
    "ensemble_block4 = EnsembleBlock(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapley_values = explainer_block.fit_transform(X,y,X_train,y_train,X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_labels = cluster_block.cluster_training_instances(shapley_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble_block.train(X_train,X_val,y_train,y_val,cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test,y_test = processing_block.prepare_test_data(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shapley_test = explainer_block.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_labels_test = cluster_block.cluster_test_instances(shapley_values,shapley_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = ensemble_block.predict(X_test,cluster_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score = metrics.mean_squared_error(y_test,y_pred)\n",
    "#score = math.sqrt(score)\n",
    "#score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = S1_Branch_Pipeline(processing_block,explainer_block,cluster_block,ensemble_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y1_pred = s1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = S2_Branch_Pipeline(processing_block2,explainer_block2,cluster_block2,ensemble_block2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y2_pred = s2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s7 = S7_Branch_Pipeline(processing_block3,explainer_block3,cluster_block3,ensemble_block3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y3_pred = s3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s10 = S10_Branch_Pipeline(processing_block4,explainer_block4,reduce_block4,cluster_block4,ensemble_block4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score1 = math.sqrt(metrics.mean_squared_error(y_test,y1_pred))\n",
    "#score2 = math.sqrt(metrics.mean_squared_error(y_test,y2_pred))\n",
    "#score3 = math.sqrt(metrics.mean_squared_error(y_test,y3_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#55.3494 -> score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar(range(3),[score1,score2,score3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openml.config.apikey = '7be8de439f25368679e0802040791d1f'\n",
    "sk_learn_ext = extensions.sklearn.SklearnExtension()\n",
    "#task = tasks.get_task(52948)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_s1 = sk_learn_ext.model_to_flow(s1)\n",
    "#flow_s1.components['Building-Box-Model'].description = 'Thesis work v0.1'\n",
    "flow_s1.components['processing_block'].description = 'Pre-process Block'\n",
    "flow_s1.components['explainer_block'].description = 'Explainer Model'\n",
    "flow_s1.components['cluster_block'].description = 'Clustering Models'\n",
    "flow_s1.components['ensemble_block'].description = 'Ensemble Model'\n",
    "flow_s1.description = s1.description\n",
    "flow_s1\n",
    "#run_s1 = runs.run_flow_on_task(flow_s1,task,avoid_duplicate_runs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_s2 = sk_learn_ext.model_to_flow(s2)\n",
    "flow_s2.components['processing_block'].description = 'Pre-process Block'\n",
    "flow_s2.components['explainer_block'].description = 'Explainer Model'\n",
    "flow_s2.components['cluster_block'].description = 'Clustering Models'\n",
    "flow_s2.components['ensemble_block'].description = 'Ensemble Model'\n",
    "flow_s2.description = s2.description\n",
    "flow_s2.components\n",
    "#run_s2 = runs.run_flow_on_task(flow_s2,task,avoid_duplicate_runs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_s7 = sk_learn_ext.model_to_flow(s7)\n",
    "flow_s7.components['processing_block'].description = 'Pre-process Block'\n",
    "flow_s7.components['explainer_block'].description = 'Explainer Model'\n",
    "flow_s7.components['cluster_block'].description = 'Clustering Models'\n",
    "flow_s7.components['ensemble_block'].description = 'Ensemble Model'\n",
    "flow_s7.description = s7.description\n",
    "flow_s7.components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_s10 = sk_learn_ext.model_to_flow(s10)\n",
    "flow_s10.components['processing_block'].description = 'Pre-process Block'\n",
    "flow_s10.components['explainer_block'].description = 'Explainer Model'\n",
    "flow_s10.components['cluster_block'].description = 'Clustering Models'\n",
    "flow_s10.components['ensemble_block'].description = 'Ensemble Model'\n",
    "flow_s10.components['reduce_block'].description = 'Dimensionality Reduction Model'\n",
    "flow_s10.description = s10.description\n",
    "flow_s10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#score1 = run_s1.get_metric_fn(metrics.mean_squared_error).min()\n",
    "\n",
    "#score2 = run_s2.get_metric_fn(metrics.mean_squared_error).min()\n",
    "\n",
    "#score7 = run_s7.get_metric_fn(metrics.mean_squared_error).min()\n",
    "\n",
    "#score10 = run_s10.get_metric_fn(metrics.mean_squared_error).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.bar(range(4),[score1,score2,score7,score10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_dict = openml.tasks.list_tasks(task_type_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_list = pd.DataFrame.from_dict(task_dict, orient='index') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_list =task_list[task_list['NumberOfInstancesWithMissingValues'] == 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#211692 -> analcat_data - regression-holdout set\n",
    "#211693 -> detroit - regression-holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_list.head(20)\n",
    "task_no = 211696\n",
    "tasks.get_task(task_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "m1_scores = []\n",
    "for i in [1]:\n",
    "       #   ,2,3,4,5,8,11,12]:\n",
    "    print('Running task {}'.format(i))\n",
    "    run = runs.run_flow_on_task(flow_s1, tasks.get_task(task_no),avoid_duplicate_runs = False)\n",
    "    #myrun = run.publish()\n",
    "    score = run.get_metric_fn(metrics.mean_squared_error)\n",
    "    m1_scores.append(score.mean())\n",
    "#myrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_scores = []\n",
    "for i in [1]:\n",
    "#,2,3,4,5,8,11,12]:\n",
    "    print('Running task {}'.format(i))\n",
    "    run = runs.run_flow_on_task(flow_s2, tasks.get_task(task_no),avoid_duplicate_runs = False)\n",
    "    #myrun = run.publish()\n",
    "    score = run.get_metric_fn(metrics.mean_squared_error)\n",
    "    m2_scores.append(score.mean())\n",
    "#myrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m7_scores = []\n",
    "for i in [1]:\n",
    "#,2,3,4,5,8,11,12]:\n",
    "    print('Running task {}'.format(i))\n",
    "    run = runs.run_flow_on_task(flow_s7, tasks.get_task(task_no),avoid_duplicate_runs = False)\n",
    "    #myrun = run.publish()\n",
    "    score = run.get_metric_fn(metrics.mean_squared_error)\n",
    "    m7_scores.append(score.mean())\n",
    "#myrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m10_scores = []\n",
    "for i in [1]:\n",
    "          #,2,3,4,5,8,11,12]:\n",
    "    print('Running task {}'.format(i))\n",
    "    run = runs.run_flow_on_task(flow_s10, tasks.get_task(task_no),avoid_duplicate_runs = False)\n",
    "    #myrun = run.publish()\n",
    "    score = run.get_metric_fn(metrics.mean_squared_error)\n",
    "    m10_scores.append(score.mean())\n",
    "#myrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "m1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame([m1_scores,m2_scores,m7_scores,m10_scores],index = ['S1','S2', 'S7', 'S10']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(y = score_df.index,hue = score_df.columns,data = score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_melted = pd.melt(score_df)\n",
    "#score_melted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.barplot(y = 'value',hue = 'variable',data = score_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,name = datasets.returnDataset(26)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "C:\\Users\\koral\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "for i in range(24,26):\n",
    "    for nC in [1,2,3,4,5]:\n",
    "        timings = []\n",
    "        X,y,name = datasets.returnDataset(i)\n",
    "        #X = preprocessing.StandardScaler().fit_transform(X)\n",
    "        #y = preprocessing.StandardScaler().fit_transform(y.reshape(-1,1))\n",
    "        #X = pd.DataFrame(X)\n",
    "        #X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.25,random_state = 42)\n",
    "        #X_train.reset_index(inplace = True)\n",
    "        #X_train.drop(['index'],axis = 1,inplace = True)\n",
    "        #X_test.reset_index(inplace = True)\n",
    "        #X_test.drop(['index'],axis = 1,inplace = True)\n",
    "\n",
    "        X_train,X_test,y_train,y_test = prepare_pipeline_data(X,y)\n",
    "        notebook_mode = 'Original'\n",
    "        explainer_type = 'XGBoost'\n",
    "        model_type = 'XGBoost'\n",
    "        nClusters = nC\n",
    "\n",
    "        processing_block1,explainer_block1,cluster_block1,ensemble_block1 = create_blocks(elems = 4)\n",
    "        explainer_block1.explainer_type = 'XGBoost'\n",
    "        \n",
    "        processing_block2,explainer_block2,cluster_block2,ensemble_block2 = create_blocks(elems = 4)\n",
    "        \n",
    "        processing_block6,explainer_block6,cluster_block6,ensemble_block6 = create_blocks(elems = 4)\n",
    "        \n",
    "        processing_block7,explainer_block7,cluster_block7,ensemble_block7 = create_blocks(elems = 4)\n",
    "        \n",
    "        processing_block10,explainer_block10,reduce_block10,cluster_block10,ensemble_block10 = create_blocks(elems = 5)\n",
    "        \n",
    "        processing_block11,explainer_block11,reduce_block11,cluster_block11,ensemble_block11 = create_blocks(elems = 5)\n",
    "\n",
    "        s1 = S1_Branch_Pipeline(processing_block1,explainer_block1,cluster_block1,ensemble_block1)\n",
    "        s2 = S2_Branch_Pipeline(processing_block2,explainer_block2,cluster_block2,ensemble_block2)\n",
    "        s6 = S6_Branch_Pipeline(processing_block6,explainer_block6,cluster_block6,ensemble_block6)\n",
    "        s7 = S7_Branch_Pipeline(processing_block7,explainer_block7,cluster_block7,ensemble_block7)\n",
    "        s10 = S10_Branch_Pipeline(processing_block10,explainer_block10,reduce_block10,cluster_block10,ensemble_block10)\n",
    "        s11 = S11_Branch_Pipeline(processing_block11,explainer_block11,reduce_block11,cluster_block11,ensemble_block11)\n",
    "\n",
    "        start_time = time.time()\n",
    "        s1.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s1_y = s1.predict(X_test)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        s2.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s2_y = s2.predict(X_test)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        s6.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s6_y = s6.predict(X_test)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        s7.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s7_y = s7.predict(X_test)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        s10.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s10_y = s10.predict(X_test)\n",
    "        \n",
    "        \n",
    "        start_time = time.time()\n",
    "        s11.fit(X_train,y_train)\n",
    "        timings.append(time.time() - start_time)\n",
    "        s11_y = s11.predict(X_test)\n",
    "        \n",
    "        \n",
    "        s1_err = math.sqrt(metrics.mean_squared_error(y_test,s1_y))\n",
    "        s2_err = math.sqrt(metrics.mean_squared_error(y_test,s2_y))\n",
    "        s6_err = math.sqrt(metrics.mean_squared_error(y_test,s6_y))\n",
    "        s7_err = math.sqrt(metrics.mean_squared_error(y_test,s7_y))\n",
    "        s10_err = math.sqrt(metrics.mean_squared_error(y_test,s10_y))\n",
    "        s11_err = math.sqrt(metrics.mean_squared_error(y_test,s11_y))\n",
    "\n",
    "        s1_params,s1_estimator,s1_avg_nodes = s1.calculate_complexity()\n",
    "        s2_params,s2_estimator,s2_avg_nodes = s2.calculate_complexity()\n",
    "        s6_params,s6_estimator,s6_avg_nodes = s6.calculate_complexity()\n",
    "        s7_params,s7_estimator,s7_avg_nodes = s7.calculate_complexity()\n",
    "        s10_params,s10_estimator,s10_avg_nodes = s10.calculate_complexity()\n",
    "        s11_params,s11_estimator,s11_avg_nodes = s11.calculate_complexity()\n",
    "        \n",
    "        values = [s1_err,s2_err,s6_err,s7_err,s10_err,s11_err]\n",
    "        f=open(\"../Data/test_six_branch.txt\", \"a+\")\n",
    "        f.write(name + ',')\n",
    "        [f.write('{0:.3f},'.format(values[v])) for v in range(len(values))]\n",
    "        f.write('{0},'.format(nClusters))\n",
    "        f.write('{0},'.format(notebook_mode))\n",
    "        f.write('{0},'.format(explainer_type))\n",
    "        f.write('{0}'.format(model_type))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        f=open(\"../Data/test_runtime.txt\", \"a+\")\n",
    "        f.write(name + ',')\n",
    "        [f.write('{0:.3f},'.format(v)) for v in timings]\n",
    "        f.write('{0}'.format(nClusters))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "        params = [s1_params,s1_estimator,s1_avg_nodes,s2_params,s2_estimator,s2_avg_nodes,\n",
    "                  s6_params,s6_estimator,s6_avg_nodes,s7_params,s7_estimator,s7_avg_nodes,\n",
    "                  s10_params,s10_estimator,s10_avg_nodes,s11_params,s11_estimator,s11_avg_nodes]\n",
    "        f=open(\"../Data/test_complexity.txt\", \"a+\")\n",
    "        f.write(name + ',')\n",
    "        [f.write('{0:.3f},'.format(v)) for v in params]\n",
    "        f.write('{0}'.format(nClusters))\n",
    "        f.write('\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = openml.datasets.get_dataset(688)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt,_,_,_ = dts.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbdf = s10.explainer_block.base_model.trees_to_dataframe()\n",
    "xgbdf2 = s10.ensemble_block.model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbdf2['model0'].trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([xgbdf2['model{}'.format(i)].trees_to_dataframe().shape[0] for i in range(len(xgbdf2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_tree = s1.explainer_block.base_model.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_node_avg = s1_tree.groupby('Tree')['Node'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23190, 1546)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.calculate_complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'S1_Branch_Pipeline' object has no attribute 'n_avg_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-7eb0e959e213>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ms1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_avg_nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'S1_Branch_Pipeline' object has no attribute 'n_avg_nodes'"
     ]
    }
   ],
   "source": [
    "s1.n_avg_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(1600000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
